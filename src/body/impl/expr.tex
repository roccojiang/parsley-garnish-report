\documentclass[../../../main.tex]{subfiles}

\begin{document}

\section{Representing and Normalising Expressions}\label{sec:simplify-exprs}

% \Cref{sec:parser-representation} showed that it is useful to lift Scala \textsc{ast} nodes to a specialised \scala{Parser} \textsc{ast}, making it easier to manipulate and inspect parsers.
% Crucially, this allowed us to simplify parsers via term-rewriting rules based on parser laws.
% \Cref{sec:simplify-parsers} demonstrated why this is necessary for \texttt{parsley-garnish}: transformations such as left-recursion factoring~\cref{sec:factor-leftrec} result in complex parser terms that must be simplified to be readable.

The previous \namecref{sec:simplify-parsers} demonstrated the process of simplifying the \scala{Parser} \textsc{ast}, but this is not the only syntactic structure that requires simplification.
So far, parsers such as \scala{pure} and \scala{map} still treat expressions as black boxes in the form of raw \scala{scala.meta.Term} \textsc{ast} nodes.
This is evident from where the example in \cref{sec:simplify-example} left off, where the parser itself is in a simplified form, but the function passed to \scala{map} is not:
\begin{minted}{scala}
val f = flip(compose((_ + _).curried)(identity))
\end{minted}
%
Therefore, this \namecref{sec:simplify-exprs} explores the following:
\begin{itemize}
  \item How expressions can be represented as another intermediate \textsc{ast}, so that they are statically inspectable enough to be simplified.
  \item The idea of \emph{normalisation} to reduce expressions into a semantically equivalent but syntactically simpler form.
\end{itemize}

\subsection{The $n$-ary Lambda Calculus}
Once again, the complexity of manipulating the generic Scalameta \textsc{ast} can be avoided by building a new intermediate \textsc{ast} representation for expression terms.

Scala, as a functional programming language, uses an extension of the $\lambda$-calculus~\cite{church_lambda_1936} as its theoretical foundations~\cite{cremet_core_2006,amin_essence_2016}.
The expression terms present within parsers are equivalent to $\lambda$-terms, just with extra syntactic sugar.
In the standard $\lambda$-calculus, each function only takes one argument, and multi-argument functions are represented as a chain of single-argument functions: this is known as \emph{currying}.
Scala supports curried functions using multiple parameter lists, but uncurried functions are preferred for performance reasons.
Since these functions will be transformed from Scala code and back, it is desirable to maintain a high-level equivalence between these two representations.
Thus, the expression \textsc{ast} will be based on \cref{fig:lambda-calculus}, which extends the $\lambda$-calculus to support proper multi-argument functions using $n$-ary abstraction and application.

\begin{figure}
% annoying \enspace hack to get operators aligned along the centre
\begin{align*}
M, N &\mathrel{::=} x & \text{variable} \\
&\mathrel{\enspace\mid\enspace} (\lambda \overline{\mathbf{x}}.\ M) & \text{$n$-ary abstraction, where } \overline{\mathbf{x}} = (x_1, \ldots, x_n) \\
&\mathrel{\enspace\mid\enspace} (M\ \overline{\mathbf{N}}) & \text{$n$-ary application, wher } \overline{\mathbf{N}} = (N_1, \ldots, N_n)
\end{align*}
\caption{Syntax for the untyped $\lambda$-calculus extended with $n$-ary abstraction and application.}
\label{fig:lambda-calculus}
\end{figure}

\subsubsection{$\beta$-Reduction and $\alpha$-Conversion}
In the $\lambda$-calculus, terms are evaluated via $\beta$-reduction: \cref{fig:beta-reduction} shows how this can be defined for the $n$-ary $\lambda$-calculus.
Unlike the standard $\lambda$-calculus, reduction will only take place if the expected number of arguments in $\overline{\mathbf{x}}$ are equal to the number of arguments in $\overline{\mathbf{N}}$; otherwise, evaluation is stuck.

\begin{figure}[htbp]
\begin{equation*}
(\lambda \overline{\mathbf{x}} .\ M)\ \overline{\mathbf{N}} \rightarrow_\beta M[\overline{\mathbf{N}}/\overline{\mathbf{x}}] \hspace{3em} (\text{if } | \overline{\mathbf{x}} | = | \overline{\mathbf{N}} |)
\end{equation*}
\caption{The $\beta$-reduction rule for the $n$-ary lambda calculus.}
\label{fig:beta-reduction}
\end{figure}

The syntax $M[N/x]$ denotes term substitution, where all free occurrences of $x$ in $M$ are replaced with $N$.
Substitution must avoid \emph{variable capture}, when $N$ contains free variables that are bound in the scope where $x$ is found~\cite{van-bakel_tsfpl_2022}.
Avoiding capture is achieved by performing $\alpha$-conversion, which is the process of renaming bound variables.
In the $\lambda$-calculus, two terms are considered $\alpha$-equivalent if they can be transformed into each other by only renaming bound variables: the term $\lambda x. x$ is equivalent to $\lambda y. y$.

\paragraph{Illustrating variable capture}
For example, substitution without $\alpha$-conversion incorrectly $\beta$-reduces the following term:
\begin{align*}
(\lambda x. \lambda y. x y) y\ &\rightarrow_\beta (\lambda y. x y)\ [y / x] \\
&= \lambda y. y y
\end{align*}
The $y$ that was substituted was originally a free variable, distinct from the $y$ bound in the lambda $\lambda y. x y$.
% some weird spacing hacks because $y$ is weirdly slanted
However, after substitution, it became captured under the lambda, where the two $y\ $ terms are now indistinguishable in the incorrect expression $\lambda y. y y$.
The correct $\beta$-reduction with capture-avoiding substitution would instead proceed as follows:
\begin{align*}
(\lambda x. \lambda y. x y) y\ &\rightarrow_\beta (\lambda y. x y)\ [y / x] \\
&=_\alpha (\lambda z. x z)\ [y / x] \\
&= \lambda z. y z
\end{align*}

\subsubsection{Simplifying the Example Expression}
The example from the beginning of the \namecref{sec:simplify-exprs} can thus be evaluated by hand via $\beta$-reduction, representing the higher-order functions as $\lambda$-abstractions:
\begin{align*}
\mathtt{flip(compose((\_ + \_).curried)(identity))}\quad &\mathrel{=\quad\,} \mathrm{flip}\ (\mathrm{compose}\ (\lambda a. \lambda b. a + b)\ \mathrm{identity}) \\
&\mathrel{=\quad\,} \mathrm{flip}\ ( (\lambda f. \lambda g. \lambda x. f\ (g\ x)) (\lambda a. \lambda b.\ a + b) (\lambda x. x)) \\
&\rightarrow_{\beta*} \mathrm{flip}\ (\lambda g. \lambda x.\ (\lambda b.\ g\ x + b) (\lambda x. x)) \\
&\rightarrow_{\beta*} \mathrm{flip}\ (\lambda x. \lambda b.\ x + b) \\
&\mathrel{=\quad\,} (\lambda f. \lambda x. \lambda y.\ f\ y\ x) (\lambda x. \lambda b.\ x + b) \\
&\rightarrow_{\beta*} \lambda x. \lambda y.\ y + x \\
&\mathrel{=\quad\,} \mathtt{(x, y) => y + x}
\end{align*}
%
This normalised expression has the same meaning as the original, but is now expressed in a clearer form!
This is much more suitable to be placed back into the code rewrite, as it is easier to understand by the user.
The rest of the \namecref{sec:simplify-exprs} now explores how this process can be implemented in \texttt{parsley-garnish}.

\subsection{Representing Names}
There are a plethora of approaches to implementing the $\lambda$-calculus, mostly differing in how they represent variable names.
This affects how variable capture is handled, and also how $\alpha$-equivalence of two terms can be determined.
For \texttt{parsley-garnish}, cheap $\alpha$-equivalence is desirable to help check equivalence of parser terms, which is useful for some transformations.

\paragraph{Naïve capture-avoiding substitution}
Representing variable names as strings is the most straightforward approach in terms of understandability.
The example below shows how the simply typed $\lambda$-calculus can be represented as a generalised algebraic data type (\textsc{gadt})~\cite{cheney_gadt_2003} in Scala:

\begin{minted}{scala}
type VarName = String

trait Lambda
case class Abs[A, B](x: Var[A], f: Lambda[B]) extends Lambda[A => B]
case class App[A, B](f: Lambda[A => B], x: Lambda[A]) extends Lambda[B]
case class Var[A](name: VarName) extends Lambda[A]

// λf. λx. f x
val f = Var("f"), val x = Var("x")
val expr = Abs(f, Abs(x, App(f, x)))
\end{minted}
%
Although naïvely substituting these terms seems logically simple, it can be very tricky to get right.
This approach requires calculating the free variables in a scope before performing substitution, renaming bound variables if it would lead to variable capture.
Due to the inefficiency of having to traverse the whole term tree multiple times, this approach is not used in any real implementation of the $\lambda$-calculus.
Furthermore, checking $\alpha$-equivalence is also tedious, requiring another full traversal of the term tree to compare variable names.

\paragraph{Barendregt's convention}
Renaming all bound variables to be unique satisfies \emph{Barendregt's convention}~\cite{barendregt_lambda_1984}, which removes the need to check for variable capture during substitution.
However, to maintain this invariant, variables must also be renamed during substitution -- this administrative renaming incurs a relatively high performance overhead and chews through a scarily large number of fresh variable names.
The approach has been successfully optimised to very impressive performance, though:
the Haskell \textsc{ghc} compiler uses Barendregt's convention with a technique dubbed ``the Rapier''~\cite{peytonjones_secrets_2002}, maintaining further invariants to avoid renaming on substitution when unnecessary.
Unfortunately, maintaining the invariants to keep this transformation correct becomes very difficult~\cite{maclaurin_thefoil_2023}.

\paragraph{Nameless and hybrid representations}
Nameless representations like \emph{De Bruijn indices}~\cite{debruijn_lambda_1972} eschew names entirely, instead representing variables as the number of binders between the variable and its binding site.
For example, $\lambda x. x(\lambda y. x y)$ would be represented as $\lambda. 0(\lambda. 1\ 0)$: the variable $y$ is replaced with $0$ to signify that $y$ is bound by the closest $\lambda$ binder.
The first occurrence of $x$ is replaced with $0$ in the same way, but its second occurrence is replaced with $1$ since it is bound by the $\lambda$ two levels up.

An advantage of De Bruijn indexing is that it makes $\alpha$-equivalence trivial to check, as it is just a matter of comparing the indices.
However, although an elegant representation, De Bruijn terms are notoriously difficult to work with as they are not easily human-readable.
% https://proofassistants.stackexchange.com/questions/900/when-should-i-use-de-bruijn-levels-instead-of-indices
Furthermore, performing substitutions with De Bruijn terms has an overhead as variable positions have to be shifted -- this is undesirable given that the purpose of the \textsc{ast} is to normalise $\lambda$-terms.
To avoid this, hybrid representations combining named and nameless representations exist~\cite{mcbride_imnotanumber_2004,chargueraud_locally_2012}, but they become rather complex solutions for what should be a relatively simple $\lambda$-calculus implementation for \texttt{parsley-garnish}'s needs.

\paragraph{Higher-order abstract syntax}
Using \emph{higher-order abstract syntax} (\textsc{hoas})~\cite{pfenning_hoas_1988} sidesteps variable binders entirely by borrowing substitution from the meta-language, making it the meta-language's responsibility to handle variable capture instead.
In contrast, the previous techniques were examples of first-order abstract syntax, which represents variables and unknowns with identifiers (whether with names or indices).
A \textsc{hoas} approach does not name bound variables, instead representing them as bindings in the meta-language:

\begin{minted}{scala}
trait HOAS
case class Abs[A, B](f: HOAS[A] => HOAS[B]) extends HOAS[A => B]
case class App[A, B](f: HOAS[A => B], x: HOAS[A]) extends HOAS[B]

// λf. λx. f x
val expr = Abs(f => Abs(x => App(f, x)))
\end{minted}
%
Therefore, this representation performs substitution through Scala's function application, which makes it extremely fast compared to the other approaches.
However, since lambda abstractions are represented as closures within Scala itself, the function body becomes wrapped under Scala's variable bindings, making them difficult to inspect and work with.

% \scala{Expr} has one binding construct -- the n-ary abstraction. This introduces variables and the need for capture-avoiding substitution.

\subsection{Normalisation Strategies}\label{sec:normalisation-approach}
One remaining hurdle stands before deciding on an \textsc{adt} representation: how normalisation will be implemented.
The ideas of partial evaluation and normalisation are related concepts -- it is useful to view normalisation as statically evaluating as many terms as possible, but since not all terms have known values, the expression cannot be fully evaluated to a result value.
Normalisation can thus be viewed simply as a process of evaluation, but in the presence of unknown terms.
This \namecref{sec:normalisation-approach} briefly explains the traditional notion of reduction-based normalisation, before introducing normalisation by evaluation as a more elegant and efficient strategy.

\subsubsection{Reduction-Based Normalisation}
The $\beta$-reduction rule is a \emph{directed} notion of reduction, which can be implemented as a syntax-directed term-rewriting system, in a similar way to how \scala{Parser} terms are simplified.
The goal is to achieve beta normal form ($\beta$-\textsc{nf}) by allowing $\beta$-reduction to occur deep inside $\lambda$-terms, in all redexes of a term, until no more reductions can be made.
% Normally in the untyped $\lambda$-calculus, $\beta$-reduction does not manage to be strongly normalising as desired, and may lead to non-terminating rewrites.
% However, this is not an issue for us: despite the \scala{Lambda} representation itself permitting untyped terms, all \scala{Lambda} terms are converted from valid typed Scala expressions.
% Scala 3 is modelled on the Dependent Object Types (\textsc{dot}) calculus, which has been shown to be strongly normalisable~\cite{wang_strong_2017}.
% Although not proven for Scala 2, it is reasonable to assume that the same result holds, at least for the subset of expressions that are represented in the \scala{Lambda} representation.

\subsubsection{Normalisation by Evaluation}
An interesting alternative strategy stems from a notion of \emph{reduction-free} normalisation, based on an undirected notion of term equivalence, rather than directed reduction.
\emph{Normalisation by evaluation} (\textsc{nbe})~\cite{filinski_nbe_2004} achieves this by \emph{evaluating} syntactical terms into a semantic model, then \emph{reifying} them back into the syntactic domain.
The denotational model (denoted by $\text{\textlbrackdbl} - \text{\textrbrackdbl}$) generally involves implementing a separate datatype from the syntactic \textsc{ast} representation of functions.
The semantics is specifically constructed to be \emph{residualising}, meaning that terms can be extracted out into the original syntactic representation.
Normalisation is then just defined as the composition of these two operations, as illustrated in \cref{fig:nbe}.

\begin{figure}
\begin{equation*}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB12cYAPHYAMoBPMDjo8AviAml0mXPkIoATOSq1GLNp259BMALZ1RWAMZwpE9TCgBzeEVAAzAE4QDSMiBwQkARmo4AAssJxwkAFpVDWZWRA4uXn4dJIYAIxc6UwBrKDSGCQACCMKUvTKcFwys3PzLWRBXd39qH09AkLDI6IY6NJgGAAV5PAI2Bhgu6npY7US+XGAXGFChSwoJIA
\begin{tikzcd}
  \text{Syntactic domain} \arrow[rr, "\text{\textlbrackdbl} - \text{\textrbrackdbl}", shift left=2] &  & \text{Semantic domain} \arrow[ll, "\textit{reify}", shift left=2]
\end{tikzcd}
\end{equation*}
\vspace{0.5ex}
\begin{equation*}
\textit{normalise} = \textit{reify} \circ \text{\textlbrackdbl} - \text{\textrbrackdbl}
\end{equation*}
\caption{Normalisation by evaluation in a semantic model.}
\label{fig:nbe}
\end{figure}

\subsection{The Expression \textsc{adt}}
The final implementation of the \scala{Expr} \textsc{ast} normalises terms with \textsc{nbe}, which results in a two-tiered representation of expression terms:
\begin{enumerate}
  \item Scalameta \textsc{ast} nodes corresponding to expressions are lifted to the \scala{Expr} \textsc{adt}, which represents the syntax of lambda expressions using a simple named approach.
  \item \scala{Sem} uses \textsc{hoas} to leverage Scala semantics as the denotational model for lambda expressions. During normalisation, \scala{Expr} terms are evaluated into \scala{Sem}, then reified back into \scala{Expr}.
\end{enumerate}
%
This achieves the following desired properties for \texttt{parsley-garnish}'s use cases:
\begin{itemize}
  \item The syntactic \scala{Expr} \textsc{adt} is represented in a simple manner, which is easy to construct and manipulate as opposed to a \textsc{hoas} representation. This allows function terms to be pattern matched on, as part of parser simplifications.
  \item Lifting the syntactic constructs to Scala semantics with \textsc{hoas} unlocks extremely efficient normalisation, and easier guarantees of correctness with respect to variable capture.
  \item Reifying \scala{Sem} terms back into syntactic \scala{Expr} terms automatically $\alpha$-converts names, granting $\alpha$-equivalence for free.
\end{itemize}
%
% \subsubsection{Defunctionalisation}
% In the same way that representing parsers as datatypes allows them to be statically inspectable, the aforementioned higher-order functions can also be rendered as datatypes to allow them to be analysed in a similar manner.
% This is an example of \emph{defunctionalisation}, a technique to eliminate higher-order functions by transforming them into specialised first-order equivalents~\cite{reynolds_defunc_1972,danvy_defunctionalization_2001}.
%
% Ideally for type safety, this could be represented as a generalised algebraic data type (\textsc{gadt})~\cite{cheney_gadt_2003} with a type parameter documenting the type that the function term represents:
% \begin{minted}{scala}
% trait Defunc[+T]
% case class Identity[A]() extends Defunc[A => A]
% case class Flip[A, B, C]() extends Defunc[(A => B => C) => B => A => C]
% case class Compose[A, B, C]() extends Defunc[(B => C) => (A => B) => A => C]
% \end{minted}
%
\Cref{fig:expr-adt} shows the implementation of the untyped \scala{Expr} \textsc{adt} representing the abstract syntax of $n$-ary $\lambda$-terms, extended with the following:
\begin{itemize}
  \item Optional explicit type annotations for variables -- these are not used for type-checking, but are there to preserve Scala type annotations originally written by the user.
  \item \scala{Translucent} terms to encapsulate open terms holding a \scala{scala.meta.Term} which cannot be normalised further. These carry an environment of variable bindings to substitute back in during pretty-printing -- in a metaprogramming context, this is analogous to splicing into a quoted expression.
\end{itemize}
This structure is largely mirrored by the \textsc{hoas}-based \scala{Sem} \textsc{adt} shown in \cref{fig:sem-adt}, which allows it to be reified back into \scala{Expr} terms.

\begin{figure}
\begin{subfigure}{\textwidth}
\begin{minted}{scala}
trait Expr
case class AbsN(xs: List[Var], f: Expr) extends Expr
case class AppN(f: Expr, xs: List[Expr]) extends Expr
case class Var(name: VarName, displayType: Option[scala.meta.Type]) extends Expr
case class Translucent(t: Term, env: Map[VarName, Expr]) extends Expr
\end{minted}
\caption{The \scala{Expr} \textsc{adt} for representing the abstract syntax of lambda expressions.}
\label{fig:expr-adt}
\end{subfigure}
%
\begin{subfigure}{\textwidth}
\vspace{3ex}
\begin{minted}{scala}
trait Sem
case class Abs(paramTypes: List[Option[scala.meta.Type]], f: List[Sem] => Sem) extends Sem
case class App(f: Sem, xs: List[Sem]) extends Sem
case class Var(name: VarName, displayType: Option[scala.meta.Type]) extends Sem
case class Translucent(t: Term, env: Map[VarName, Sem]) extends Sem
\end{minted}
\caption{The \scala{Sem} \textsc{adt} for representing the residualising semantics of lambda expressions.}
\label{fig:sem-adt}
\end{subfigure}
\caption{The intermediate \textsc{ast} for expressions.}
\end{figure}

\paragraph{Constructing higher-order functions}
\scala{Expr} also implements some helper objects to make it more convenient to construct and deconstruct single-parameter abstractions and applications:
\begin{minted}{scala}
object Abs {
  // Convenience factory method to create a single-parameter abstraction
  def apply(x: Var, f: Expr) = AbsN(List(x), f)
  // Extractor allows user to pattern match on Abs(x, f)
  // instead of the more tedious AbsN(List(x), f)
  def unapply(func: AbsN): Option[(Var, Expr)] = func match {
    case AbsN(List(x), f) => Some((x, f))
    case _ => None
  }
}

object App {
  // Convenience factory method to apply a single argument to a function
  def apply(f: Expr, x: Expr) = AppN(f, List(x))
  // Apply multiple arguments in sequence, as f(x)(y)(z) instead of f(x, y, z)
  def apply(f: Expr, xs: Expr*) = xs.foldLeft(f)(App(_, _))
}
\end{minted}
%
Using these objects, \cref{fig:higher-order-funcs} shows how the higher-order functions necessary for left-recursion factoring can be implemented as constructors for \scala{Expr} terms.

\begin{figure}
\begin{minted}{scala}
/* id : A => A */
def id: Expr = {
  val x = Var.fresh()
  Abs(x, x)
}

/* flip : (A => B => C) => B => A => C */
def flip: Expr = {
  val (f, x, y) = (Var.fresh(), Var.fresh(), Var.fresh())
  Abs(f, Abs(x, Abs(y, App(f, y, x)))) // λf. λx. λy. f y x
}

/* compose : (B => C) => (A => B) => A => C */
def compose: Expr = {
  val (f, g, x) = (Var.fresh(), Var.fresh(), Var.fresh())
  Abs(f, Abs(g, Abs(x, App(f, App(g, x))))) // λf. λg. λx. f (g x)
}
def compose(f: Expr) = App(compose, f)
def compose(f: Expr, g: Expr) = App(compose, f, g)
\end{minted}
\caption{Constructors for higher-order functions represented as $\lambda$-expressions in \scala{Expr}.}
\label{fig:higher-order-funcs}
\end{figure}

% Ideally for type safety, this could be represented as a generalised algebraic data type (\textsc{gadt})~\cite{cheney_gadt_2003} with a type parameter documenting the type that the function term represents:

\paragraph{Improved type safety}
The originally intended design was to represent \scala{Expr} as a type-parameterised \textsc{gadt} for improved type safety, where it would be based on a \emph{typed} variant of the $\lambda$-calculus.
This would have also allowed \scala{Parser} to be represented as a \textsc{gadt} parameterised by the result type of the parser.
However, attempting to implement this ran into two main hurdles:
\begin{itemize}
  \item \scala{Var} and \scala{Translucent} terms would need to be created with concrete type parameters of their inferred types. Scalafix's semantic \textsc{api} is not powerful enough to guarantee that all terms can be queried for their inferred types -- in fact, the built-in Scalafix rule \emph{Explicit Result Types} calls the Scala 2 presentation compiler to extract information like this\footnote{\url{https://github.com/scalacenter/scalafix/issues/1583}}. This solution is complex and brittle due to its reliance on unstable compiler internals, which undermines Scalafix's goal of being a cross-compatible, higher-level abstraction over compiler details.
  \item Scala 2's type inference for \textsc{gadt}s is less than ideal, requiring extra type annotations and unsafe casts which ultimately defeat the original purpose of type safety. This situation is improved, although not completely solved, in Dotty~\cite{parreaux_towards_2019} -- but Scalafix does not yet support Scala 3.
\end{itemize}

\subsection{Lifting to the Intermediate Expression \textsc{ast}}\label{sec:lifting-expr}
The \scala{Parser} \textsc{ast} is amended to take \scala{Expr} arguments where they used to take \scala{scala.meta.Term} values.
Take the \scala{Pure} parser as an example:
% TODO: if time, highlight changes with tcolorbox?
\begin{minted}{scala}
case class Pure(x: Expr) extends Parser
object Pure {
  def fromTerm: PartialFunction[Term, Pure] = {
    case Term.Apply(matcher(_), Term.ArgClause(List(func), _)) => Pure(func.toExpr)
  }
}
\end{minted}
%
The \scala{toExpr} extension method on \scala{scala.meta.Term} is used to lift \scala{Term} \textsc{ast} nodes to \scala{Expr} terms.
Expression lifting is invoked whenever a parser expects an expression (whether a function or simple value) as an argument.
This \namecref{sec:lifting-expr} gives a high-level overview of the three cases that \scala{toExpr} handles.

\subsubsection{Lambda Expressions}
Writing parsers often involves defining simple lambda expressions used to glue together parsers, or to transform the result of a parser, as so:
\begin{minted}{scala}
val asciiCode: Parsley[Int] = item.map(char => char.toInt)
\end{minted}
%
These lambda expressions are represented in the Scalameta \textsc{ast} as \scala{Term.Function} nodes, which are recursively traversed to collect all parameter lists.
This is folded into a chain of $n$-ary abstractions, with the final term being the body of the lambda, which is wrapped into a \scala{Translucent} term.

To ensure that the parameter names in the \scala{Translucent} body term are unique, the parameters are $\alpha$-converted to fresh names.
The body is also transformed to make sure references to these bound variables use their new names: this conversion is well-scoped as it compares terms using their unique Scalameta symbols.
The following example illustrates when this is necessary:
% this is the "gensym" approach
\begin{minted}{scala}
a => (a, b) => a + b
\end{minted}
Although no sane Scala programmer would write this, this lambda demonstrates how it is possible to shadow variables -- the \scala{a} in the function body refers only to the \scala{a} in the second parameter list, as it shadows the \scala{a} in the first parameter list.
The lifted \scala{Expr} term would then resemble the following $\lambda$-calculus expression, where \textbf{bold} values correspond to \scala{scala.meta.Term} nodes as opposed to \scala{Expr} values:
\begin{lstlisting}
\(x1). \(x2, x3). Translucent(%\textbf{x2 + x3}%, env = {%\textbf{x1}% -> x1, %\textbf{x2}% -> x2, %\textbf{x3}% -> x3})
\end{lstlisting}
This shows how the lambda body's environment maps \scala{Term.Name} nodes to their corresponding variable terms.
When the term is pretty-printed, each \scala{Term.Name} node is replaced with their corresponding \scala{Expr} term -- this is analogous to the splicing operation on quasiquotes:
\begin{minted}{scala}
q"x1 => (x2, x3) => $x2 + $x3"
\end{minted}
% TODO: discussions on hygiene

\subsubsection{Placeholder Syntax}
Scala supports a placeholder syntax using underscores to make lambda expressions more concise, so the earlier parser can be rewritten as:
\begin{minted}{scala}
val asciiCode: Parsley[Int] = item.map(_.toInt)
\end{minted}
%
Scalameta differentiates between regular lambda expressions and those using placeholder syntax, representing the latter as \scala{Term.AnonymousFunction} nodes.
This makes it easy to identify which approach to be taken during conversion.
To convert this case, each successive underscore in the expression body is replaced with a fresh variable name.
Placeholder syntax creates a fully uncurried function with a single parameter list\footnote{\url{https://www.scala-lang.org/files/archive/spec/2.13/06-expressions.html#anonymous-functions}}.
Therefore, the converted \scala{Expr} term is always a single $n$-ary abstraction, where the arguments are the freshly generated variable names in order of their occurrence in the expression body.

\subsubsection{Eta-Expansion}
If the term is not a lambda expression, \texttt{parsley-garnish} attempts to $\eta$-expand the term if possible.
For example, an idiomatic parser written using the \emph{Parser Bridges} pattern~\cite{willis_design_2022} could resemble the following:
\begin{minted}{scala}
case class AsciiCode(code: Int)
object AsciiCode extends ParserBridge1[Char, AsciiCode] {
  def apply(char: Char): AsciiCode = AsciiCode(char.toInt)
}
val asciiCode = AsciiCode(item)
\end{minted}
%
When \texttt{parsley-garnish} converts \scala{asciiCode} to a \scala{Parser}, it desugars the bridge constructor into something resembling \scala{item.map(AsciiCode.apply)}.
The $\eta$-expanded form of \scala{AsciiCode.apply} would be as follows:
\begin{minted}{scala}
(char: Char) => AsciiCode.apply(char)
\end{minted}
%
To $\eta$-expand \scala{scala.meta.Term} nodes, \texttt{parsley-garnish} attempts to look up the method signature of its symbol using Scalafix's semantic \textsc{api}.
This is not always possible -- in that case, the term cannot be statically inspected any further and is just wrapped in a \scala{Translucent} term.

\subsection{Normalising Expression Terms}
Using \textsc{nbe}, normalisation therefore follows a two-step process: \scala{Expr} values \scala{evaluate} into \scala{Sem} values, which are then \scala{reify}ed back into \scala{Expr}:
\begin{minted}{scala}
trait Expr {
  def normalise: Expr = this.evaluate.reify
}
\end{minted}
%
% TODO: explain code
\paragraph{Evaluation}
Evaluation proceeds by carrying an environment mapping bound variables to their semantic representations.
Evaluating a variable looks up its name in the environment, while evaluating a lambda abstraction produces a closure using the current environment -- using \textsc{hoas} allows these closures to be represented as native Scala closures.
The interesting case is evaluating function application: this allows $\beta$-reduction within the \emph{semantic domain} at any point within the term, not just on the head term.
The function and its arguments are first evaluated separately -- then, if the function evaluates to an abstraction, the arguments are passed to the Scala closure \scala{g: List[Sem] => Sem}, collapsing the term structure by one step.
%
\begin{minted}{scala}
trait Expr {
  def evaluate: Sem = {
    def eval(func: Expr, boundVars: Map[Var, Sem]): Sem = func match {
      case v @ Var(name, displayType) =>
        boundVars.getOrElse(v, Sem.Var(name, displayType))
      case AbsN(xs, f) =>
        Sem.Abs(xs.map(_.displayType), vs => eval(f, boundVars ++ xs.zip(vs)))
      case AppN(f, xs) => eval(f, boundVars) match {
        case Sem.Abs(_, g) => g(xs.map(eval(_, boundVars)))
        case g => Sem.App(g, xs.map(eval(_, boundVars)))
      }
      case Translucent(term, env) =>
        Sem.Translucent(term, env.mapValues(eval(_, boundVars)))
    }

    eval(this, Map.empty)
  }
}
\end{minted}
%
\paragraph{Reification}
% https://williamjbowman.com/tmp/nbe-four-ways/#%28part._choice-11%29 also references this intensional vs extensional nomenclature
The \textsc{nbe} semantics utilised by \texttt{parsley-garnish} are \emph{intensional}~\cite{lindley_normalisation_2005}, meaning that once syntactic terms are fully evaluated into their semantics, the expression is normalised to $\beta$-\textsc{nf}.
Reification is then a simple process of converting each level of the term back into its syntactic counterpart.
When a lambda abstraction is reified, bound variables are assigned names from a fresh name supply.
This step is what grants $\alpha$-equivalence for free, as the fresh name generator can be made deterministic: given two terms that evaluate to the same semantic structure, reifying both will yield syntactic representations with the same names.
%
\begin{minted}{scala}
trait Sem {
  def reify: Expr = {
    def reify0(func: Sem)(implicit freshSupply: Fresh): Expr = func match {
      case Abs(tpes, f) =>
        val params = tpes.map(Expr.Var(freshSupply.next(), _))
        Expr.AbsN(params, reify0(
          f(params.map { case Expr.Var(name, tpe) => Sem.Var(name, tpe) } )
        ))
      case App(f, xs) => Expr.AppN(reify0(f), xs.map(reify0))
      case Translucent(t, env) => Expr.Translucent(t, env.mapValues(reify0))
      case Var(name, displayType) => Expr.Var(name, displayType)
    }

    reify0(this)(new Fresh)
  }
}
\end{minted}

\subsection{Lowering Back to the Scalameta \textsc{ast}}
Once any transformations on the \scala{Expr} terms are complete, they need to be converted back to strings to be placed in a Scalafix patch.
This is achieved in the same way as parsers in \cref{sec:lowering-parsers}, by lowering them back as \scala{scala.meta.Term} \text{ast} nodes and re-using Scalameta's pretty-printer to generate a syntactically well-formed string representation.
These can also be achieved using quasiquotes:
\begin{itemize}
  \item Lambda abstractions are transformed into a lambda expression of form \scala{q"(...params) => body"}.
  \item Function application is transformed into method calls.
  \item Variables are simply \scala{Term.Name} nodes with their syntactic names.
  \item Translucent terms splice their environment bindings back into their term body.
\end{itemize}

\subsection*{Discussion}
\texttt{parsley} Haskell, as a \emph{staged} parser combinator library, also has the ability to inspect and optimise the code of user-defined functions.
The approach taken by \texttt{parsley-garnish} and \text{parsley} share many similarities, both using the $\lambda$-calculus as a core language to normalise expressions.
In both cases, the need to reduce expression terms is motivated by how parser simplifications involve fusion, which results in function applications that can be partially evaluated.

However, the two have different motivations and requirements for normalising expressions, so their approaches differ in some ways --
\cref{fig:nbe-vs-parsley} illustrates these differences.

\paragraph{Syntactic representation}
Unlike \texttt{parsley-garnish}, \texttt{parsley} has a two-level syntactic representation for expressions.
\haskell{Defunc} is akin to a deep embedding of higher-order functions, representing them as a \textsc{gadt}: this process is known as \emph{defunctionalisation}~\cite{reynolds_defunc_1972,danvy_defunctionalization_2001}.
This helps facilitate certain parser law optimisations which require pattern matching on functions as well as parsers, for example:
\begin{equation*}
\text{\scala{pure(identity) <*> u = u}}
\end{equation*}
After this step, \haskell{Defunc} values are then brought into the lower-level $\lambda$-calculus representation \haskell{Lambda}, to be normalised by $\beta$-reduction.

At the moment, \texttt{parsley-garnish} does not have a need to implement any parser simplifications based on these laws, although this may change in the future.
Adding an extra defunctionalised layer to the expression \textsc{ast} would be fairly straightforward.

\paragraph{Normalisation strategy}
\texttt{parsley} normalises terms to full $\eta\beta$-\textsc{nf}, whereas \texttt{parsley-garnish} only normalises to $\beta$-\textsc{nf}.
This is because $\eta$-reduction in Scala 2 is not as straightforward as in Haskell, and is not always possible -- in most cases the appropriate reduction is instead to convert lambdas to placeholder syntax.
This is left as future work.

In \texttt{parsley}, normalisation is implemented as a reduction-based approach over the \textsc{hoas} \haskell{Lambda} datatype.
Normalisation by $\beta$-reduction with Haskell function application brings this to $\beta$-\textsc{whnf}.
Then, code generation brings this further to $\beta$-\textsc{nf} as desired, as well as an extra step for $\eta$-reduction to put the term into full $\eta\beta$-\textsc{nf}.

The main reason why \texttt{parsley-garnish} takes a different normalisation approach is because unlike \texttt{parsley}, there is still a need for $\alpha$-equivalence checking after normalisation.
In \texttt{parsley}, the normalised forms are immediately utilised for code generation, so they can be kept as \textsc{hoas} the entire time, without representing variables with any names.
Conversely, in \texttt{parsley-garnish}, these normalised terms undergo further analysis before being transformed into code patches for pretty-printing.

% Representation as a lambda calc has allocation overhead, but greatly simplifies function evaluation via beta reduction, instead of having to deal with high-level representations of compose/id (not too bad tbh) and flip (annoying).

% TODO: scala 3 macros, squid quasiquotes?

\begin{figure}[htbp]
  \begin{equation*}
  % Created here (but modified slightly) https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDnGADx2AAJAPIBBAMoBfEJNLpMufIRQBmCtTpNW7Lj35CxUgHq6+AsADNps+djwEiZAEwaGLNok7czwAGLMwAGN7MGs5EAw7JSI1Fxo3bU9TfQAZegBbACMoejDbRQcUcnV4rQ8vPQFxGHT6MDxAuDyIhRCiYrjNdx1vfXEAT3r6XmsNGCgAc3giUAsAJwh0pGKQHAgkMi7EirMcARg3aRpGekyYRgAFVujPRhgLHBlw+cWNmjWkNS3y5L2BOZgWAs-SOIBOZ0u10KYPujxsIBeS0QTne60QKwSP16f2AAIsd2CoPB5yuUWhdweT1mCyRKNWaK+mJ6lT2wEgc1qjCwcBgkgABABePm-VkAoEg4UcQJYOaBSUs-aHSXS2Xy3b-e4EnD8kDHU4kqHKEBzLATAAWcOeNKQdI+iE2XLA5RycDN41133YyGQfIAtHzKJQZJRJEA
  % \begin{tikzcd}[sep=5em]
  %   \text{Semantics} & \text{HOAS} \arrow[rr, "\texttt{eval}"]                                                                                                                                         &  & \text{HOAS}^\text{nf} \arrow[dd, "\texttt{reify}"] \\
  %                    &                                                                                                                                                                                 &  &                                                    \\
  %   \text{Syntax}    & \text{Expr} \arrow[uu, "\texttt{reflect}"] \arrow[rr, "\substack{\texttt{normalise} \\ =\ \texttt{reify}\ \circ\ \texttt{eval}\ \circ\ \texttt{reflect}}"'] \arrow[rruu, "{\text{\textlbrackdbl} - \text{\textrbrackdbl}}", dashed] &  & \text{Lambda}                                     
  % \end{tikzcd}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAWAXVJADcBDAGwFcYkQAdDnGADxwHA4AYyb0AdAFsYOCQBUYAJ0kBfECtLpMufIRRkATNTpNW7Lj37AAorzSK1GrdjwEiAZlIAGYwxZtETm4+HGAAZRhVdU0QDBddD1IjGj8zQIsQmztFAAIACgASLgAjGXoCgFoM-hFgMAAzFQBKRxi4nTcUTyoU0wCgywFQkTEpMvEFZRVq0PyijlLZSpnahubW5w69ZABWJN8+82CrABl6SWKoeg3Y7VdtvfcD-yPB4AARGHrmMGEb9vuRD2PRML3SxyGwAAwtAYP87gkUAB2fa9MEDTJnC5XFQ5LhcPEQ4CFEplZYQ2oAdwAFmsWtFNoDkRRnmkMfxBDDYNMiXNSbIuGSqhThHVGvSnLd4p1kF5vKz+sAuHBJExGITBrhwpF6GA8MI4Cp4dLtnLkqC2UqOCq1RqQlqThBKRUTjBaDB1WEAJ56+i8I0MqVbIhyp5oy3K1WMdUzLUACSwAHNqS63R6ct7ff7HMYYFBE-AiKB6ooIJIkGQQDgIEgDDRSmAoEh3HKLf0ZoI3aZ6DwbiWy7WaNXm+H2xDBIoYFh6l6+6Xy4hK8PEJ4QIwsGB+lc4NS8yBR0c4MxinBZMIANZWwaCSDKJhYOBwnIAXkJBI7oS7LB7T64wiwijCHaHKhJO06zmoNCMPQpSMAACginQgIoSbUjggb9gurbLsQkqYSOVY1og5B4fOSAooRSAAGykQOiBUUORE7LRC57JRiAABwsUgHGMUgACcKiUCoQA
  \begin{tikzcd}[row sep=3.5em,column sep=2em]
    {\small \textit{Semantics}}         &                                                                                                                                                           &  & \text{Sem} \arrow[dd, "\texttt{reify}"]               &  &                          &  &                                                            \\
                                        &                                                                                                                                                           &  &                                                       &  &                          &  &                                                            \\
    {\small \textit{Low-Level Syntax}}  & \text{Expr} \arrow[rruu, "\texttt{evaluate}", bend left] \arrow[rr, "\substack{\texttt{normalise}\ = \\ \texttt{evaluate}\ \circ\ \texttt{reify}}"' {yshift=-1ex}, dashed] &  & \substack{\text{Expr} \\ \text{($\beta$-\textsc{nf})}} \arrow[dd]          &  & \text{Lambda} \arrow[rr, "\texttt{normalise}"' {yshift=-1ex}] &  & \substack{\text{Lambda} \\ \text{($\beta$-\textsc{whnf})}} \arrow[dd] \\
    {\small \textit{High-Level Syntax}} &                                                                                                                                                           &  &                                                       &  & \text{Defunc} \arrow[u]  &  &                                                            \\
                                        & \texttt{scala.meta.Term} \arrow[uu]                                                                                                                       &  & \substack{\texttt{scala.meta.Term} \\ \text{ ($\beta$-\textsc{nf})}} &  & \texttt{Code} \arrow[u]  &  & \substack{\texttt{Code} \\ \text{ ($\eta\beta$-\textsc{nf})}}            
    \end{tikzcd}
  \end{equation*}
  \caption{Comparison of expression normalisation in \texttt{parsley-garnish} (left) and \texttt{parsley} Haskell (right).}
  \label{fig:nbe-vs-parsley}
\end{figure}

\end{document}
